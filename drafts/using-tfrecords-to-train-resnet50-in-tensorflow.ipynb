{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1>Using TFRecords to train ResNet50 in TensorFlow</h1>\n\n\n<strong>This tutorial will explain how to train a \"transfer learning\" model using TRFrecords.</strong>\n\nAn interactive version of the notebook is available on Kaggle at:<br>\nhttps://www.kaggle.com/spiroganas/using-tfrecords-to-train-resnet50-in-tensorflow\n<hr>\n\nKaggle's [RANZCR CLiP - Catheter and Line Position Challenge](https://www.kaggle.com/c/ranzcr-clip-catheter-line-classification) provides data in the [TFRecord format](https://www.tensorflow.org/tutorials/load_data/tfrecord).  TensorFlow can quickly read data stored in the TFRecord format.  This is especially important when the model is being trained on fast GPUs or [TPUs](https://cloud.google.com/tpu) (where IO is frequently the training bottleneck).\n\nThis notebook shows how to to use TFRecords, TensorFlow and a GPU to train a ResNet50 model. \n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom io import BytesIO\nfrom imageio import imread\nfrom IPython import display\nimport cv2 as cv\nimport os\nimport random\nimport csv\nimport glob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Some constants\nIMAGE_SIZE = 300\n\n# Set this to True if you want to print arrays and other stuff that makes the notebook hard to read.\nVERBOSE = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# View the TFRecord files provided by the \"RANZCR CLiP - Catheter and Line Position Challenge\"\n!ls '/kaggle/input/ranzcr-clip-catheter-line-classification/test_tfrecords'\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 1: Load the TFRecord files into a tf.data.Dataset\n\nhttps://www.tensorflow.org/guide/data\n\nTFRecord files can be loaded into a tf.data.Dataset.\nThe dataset's map() method can then be used to preprocess the data.  https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map\n\n\n\n\nDatasets can  can be fed directly into your model's model.fit().  They can also be \"optimized\" to speed up the training process."},{"metadata":{"trusted":true},"cell_type":"code","source":"INPUT_DATA_FOLDER = '/kaggle/input/ranzcr-clip-catheter-line-classification/train_tfrecords/'\n\n\n# Get a list of the TFRecord files\nTFRecordFiles = []\nfor dirname, _, filenames in os.walk(INPUT_DATA_FOLDER):\n    for filename in filenames:\n        if filename[-6:]=='.tfrec':\n            TFRecordFiles.append(os.path.join(dirname, filename))\n\n            \n# Spilt the data into training and validation datasets                 \nSplitNumber = int(.8*len(TFRecordFiles))  \ntrain_dataset = tf.data.TFRecordDataset(TFRecordFiles[:SplitNumber])\nval_dataset = tf.data.TFRecordDataset(TFRecordFiles[SplitNumber:])\n        \n    \n    \n# I looked this up in windows explorer\n# It's the number of images in the train folder\nDATASET_SIZE = 30083\n\ntrain_size = int(0.8 * DATASET_SIZE)\nval_size = int(0.2 * DATASET_SIZE)\n\n# Create a training and a validation datasets\nfull_dataset = tf.data.TFRecordDataset(TFRecordFiles)\nfull_dataset = full_dataset.shuffle(buffer_size=1000)\ntrain_dataset = full_dataset.take(train_size) #.cache()\nval_dataset = full_dataset.skip(train_size).take(val_size) #.cache()\n        \n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 2: See what data is included in a single example\n\nYou need to create a \"feature dictionary\" that will be used to parse the TFRecord files into a format that can be used by your model.\n\nTo do that, you need to know what data is in a TFRecords, and the data's datatype.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"for raw_record in train_dataset.take(1):\n    example = tf.train.Example()\n    example.ParseFromString(raw_record.numpy())\n    if False:  # Change this to True if you want to see the data.  I turned it off because it's long and it make this notebook hard to read.\n        print(example)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 3: Create a feature dictionary\n\nThe feature dictionary should describe the data stored in the TFRecord\nFor more details, see:  https://www.tensorflow.org/tutorials/load_data/tfrecord#read_the_tfrecord_file"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_dictionary = {\n    'CVC - Abnormal': tf.io.FixedLenFeature([], tf.int64),\n    'CVC - Borderline': tf.io.FixedLenFeature([], tf.int64),\n    'CVC - Normal': tf.io.FixedLenFeature([], tf.int64),\n    'ETT - Abnormal': tf.io.FixedLenFeature([], tf.int64),\n    'ETT - Borderline': tf.io.FixedLenFeature([], tf.int64),\n    'ETT - Normal': tf.io.FixedLenFeature([], tf.int64),\n    'NGT - Abnormal': tf.io.FixedLenFeature([], tf.int64),\n    'NGT - Borderline': tf.io.FixedLenFeature([], tf.int64),\n    'NGT - Incompletely Imaged': tf.io.FixedLenFeature([], tf.int64),\n    'NGT - Normal': tf.io.FixedLenFeature([], tf.int64),\n    'StudyInstanceUID': tf.io.FixedLenFeature([], tf.string),    \n    'Swan Ganz Catheter Present': tf.io.FixedLenFeature([], tf.int64),\n    'image': tf.io.FixedLenFeature([], tf.string),\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 4: Parse the TFRecord\n\nIn this step, we parse the TFRecord data into a more useable format."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Define two parsing functions that will turn the TFRecord back into an array and a label        \ndef _parse_function(example, feature_dictionary=feature_dictionary):\n    # Parse the input `tf.train.Example` proto using the feature_dictionary.\n    # Create a description of the features.\n    parsed_example = tf.io.parse_example(example, feature_dictionary)\n    return parsed_example\n\ntrain_dataset = train_dataset.map(_parse_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)\nval_dataset = val_dataset.map(_parse_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)\nprint(train_dataset)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 5:  Print a couple images"},{"metadata":{"trusted":true},"cell_type":"code","source":"for image_features in train_dataset.take(2):\n    image = image_features['image'].numpy()\n    display.display(display.Image(data=image))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 6: Look at an image as an numpy ndarray"},{"metadata":{"trusted":true},"cell_type":"code","source":"if VERBOSE:\n    for image_features in train_dataset.take(1):\n        image = image_features['image'].numpy()\n        numpyArray = imread(BytesIO(image))\n        print(numpyArray)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 7: Look at the labels"},{"metadata":{"trusted":true},"cell_type":"code","source":"for image_features in train_dataset.take(1):\n    print('StudyInstanceUID: ', image_features['StudyInstanceUID'].numpy())\n    print('CVC - Abnormal: ', image_features['CVC - Abnormal'].numpy())\n    print('CVC - Borderline: ', image_features['CVC - Borderline'].numpy())\n    print('CVC - Normal: ', image_features['CVC - Normal'].numpy())\n    print('ETT - Abnormal: ', image_features['ETT - Abnormal'].numpy())\n    print('ETT - Borderline: ', image_features['ETT - Borderline'].numpy())\n    print('ETT - Normal: ', image_features['ETT - Normal'].numpy())\n    print('NGT - Abnormal: ', image_features['NGT - Abnormal'].numpy())\n    print('NGT - Borderline: ', image_features['NGT - Borderline'].numpy())\n    print('NGT - Incompletely Imaged: ', image_features['NGT - Incompletely Imaged'].numpy())\n    print('NGT - Normal: ', image_features['NGT - Normal'].numpy())\n    print('Swan Ganz Catheter Present: ', image_features['Swan Ganz Catheter Present'].numpy())\n    \n    \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 8: Data Augmentation\n\nData Augmentation can improve the performance of you model. It applies transformations to the real images to generate \"fake\" (but realistic) images.  This increases the size of your training data, which often increases the accuaracy of your model.\n\nThis keras model can be used to add data augmentation to the TFDataset\nhttps://www.tensorflow.org/tutorials/images/data_augmentation#two_options_to_use_the_preprocessing_layers"},{"metadata":{"trusted":true},"cell_type":"code","source":"# There are a lot of possible ways to transform an image.\n\ndata_augmentation = tf.keras.Sequential([\n        tf.keras.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3)),    \n    \n        # Randomly pick about 83% of the area of the image (1/(1.1^2))                                       \n        #tf.keras.layers.experimental.preprocessing.Resizing(height=int(1.1*IMAGE_SIZE), width=int(1.1*IMAGE_SIZE+100), interpolation='bilinear'),\n        #tf.keras.layers.experimental.preprocessing.RandomCrop(height=IMAGE_SIZE, width=IMAGE_SIZE),\n\n        # Changes the contrast.  Not required if using CT scan data that has been converted to Housfield Units.\n        #tf.keras.layers.experimental.preprocessing.RandomContrast(factor=0.1 ),\n\n        # Randomly flip the image left/right and/or up/down \n        tf.keras.layers.experimental.preprocessing.RandomFlip(),\n    \n        # Randomly rotate the image\n        tf.keras.layers.experimental.preprocessing.RandomRotation(factor=(-0.2, 0.2), fill_mode='constant'),\n], name=\"Data_Augmentation\")\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 9: Generate (feature, label) pairs\n\nThe \"features\" are the data you are using to make your prediction (i.e. the medical image)\nThe \"labels\" are what you are trying to predict.\n\nWe want to transform the TFRecord feature (a string of bytes) back into an image.  Then we want to conver the image from greyscale to RGB color, and change it's size.\nFor our labels, we just want to turn them into one long list of zeros and ones."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define two parsing functions that will turn the TFRecord back into an array and a label        \ndef generate_training_example(example):\n\n    # Convert the image to an ndarray, resize it and convert it to RGB color\n    # These are the settings most commonly required by base models used in transfer learning.\n\n    features =  tf.io.decode_image(example['image'], expand_animations = False)\n    features =  tf.image.grayscale_to_rgb(features)\n    features =  tf.image.resize(features,size=(IMAGE_SIZE,IMAGE_SIZE))\n    \n    \n    \n    #features = example['image'].numpy()\n    #features = imread(BytesIO(features))\n    #features = cv.resize(features, new_image_size)\n    #features = cv.cvtColor(features, cv.COLOR_GRAY2RGB)\n    labels = [ # Edit this to add whatever labels you want your model to predict\n                example['CVC - Abnormal'],\n                example['CVC - Borderline'],\n                example['CVC - Normal'],\n                example['ETT - Abnormal'],\n                example['ETT - Borderline'],\n                example['ETT - Normal'],\n                example['NGT - Abnormal'],\n                example['NGT - Borderline'],\n                example['NGT - Incompletely Imaged'],\n                example['NGT - Normal'],\n                example['Swan Ganz Catheter Present'],\n            ]\n\n    return features, labels\n\n\ntrain_dataset = train_dataset.map(generate_training_example, num_parallel_calls=tf.data.experimental.AUTOTUNE) # .cache('/kaggle/temp/train.cache')\nval_dataset = val_dataset.map(generate_training_example, num_parallel_calls=tf.data.experimental.AUTOTUNE) # .cache('/kaggle/temp/test.cache')\n\n\n\n\n\ntrain_dataset = train_dataset.batch(32)\nval_dataset = val_dataset.batch(32)\n\n# Apply data augmentation to the training data set\ntrain_dataset = train_dataset.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n\n\n\n\ntrain_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\nval_dataset = val_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n\n\nif VERBOSE:\n    for X in train_dataset.take(1):\n        print(X[0])\n        print('-------------------')\n        print(X[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 10: Create a ResNet50 model\n\nNothing fancy, I just want to show that the above code works.\n\nThis is an example of \"Transfer Learning\".  We are starting with an existing, trained model.  We are adding a new \"classification head\" and then training the model to predict our labels.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model():\n    \n    n_labels = 11 # number or output classes\n    \n    auc = tf.keras.metrics.AUC(multi_label=True) # metric for multi-class multi-label models\n\n\n    # https://keras.io/api/applications/\n    # Options include: ResNet50, MobileNetV2\n    base_model = tf.keras.applications.ResNet50(\n        input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3), include_top=False, weights=\"imagenet\"\n    )\n\n    base_model.trainable = True\n\n    inputs = tf.keras.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n    \n    # built-in resnet preprocessor\n    #https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet/preprocess_input\n    x = tf.keras.applications.resnet.preprocess_input(inputs)  \n    \n    x = base_model(x, training=False)\n    \n\n    # Convert features of shape `base_model.output_shape[1:]` to vectors\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    # A Dense classifier with a single unit (binary classification)\n    outputs = tf.keras.layers.Dense(n_labels, activation='sigmoid')(x)\n    model = tf.keras.Model(inputs, outputs)\n\n    model.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n    loss='binary_crossentropy',\n    #loss=custom_loss_fn,  # Custom Code to apply class weights\n    metrics=[auc])\n\n\n    return model\n\n\n\n\nmy_model = create_model()\nprint('New model created!')\nprint()\nprint(my_model.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 11: Train and Evaluate the ResNet50 model\n\nWe will use \"callbacks\"to produce a better model\n\nThe lr_reducer_callback callback will slowly reduce the learning rate as the model trains.\n\nThe EarlyStopping_callback will stop training the model once the model has achieved it's best fit."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Callback for decaying the learning rate.\nlr_reducer_callback = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor=\"val_auc\", factor=0.1, patience=3, min_lr=1e-6, mode='max',verbose=1)\n\n# Callback that stops training once the model has stopped imprtoving\nEarlyStopping_callback = tf.keras.callbacks.EarlyStopping(\n    monitor='val_auc', \n    min_delta=0.0001, \n    patience=5, \n    verbose=1,\n    mode='max', \n    restore_best_weights=True\n)\n\n\n\n\n\nhistory = my_model.fit(\n    train_dataset,\n    epochs=100,\n    validation_data=val_dataset,\n    callbacks=[lr_reducer_callback,\n                EarlyStopping_callback\n                ],\n)\n\n# Save the trained model\nmy_model.save(\"/kaggle/working/ResNet50_Saved_Model.h5\" , save_format=\"h5\")\n\n\n# test on the whole evaluiation dataset\nresults = my_model.evaluate(val_dataset)\nprint(\"Best Model Multi-label AUC: \", round(results[1],4) )\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}